<?xml version="1.0" encoding="UTF-8"?>
<document>
  <properties>
    <title>索引文字列抽出についての設定</title>
    <author>曽根 孝明</author>
  </properties>
  <body>

    <section name="索引文字列抽出について">
      <p>検索のためのインデックスを作成する際、索引として登録するために文書を切り分ける必要があります。このために使用されるのが、トークナイザーです。</p>
      <p>基本的に、トークナイザーによって切り分けられた単位よりも小さいものは、検索を行ってもヒットしません。例えば、「東京都に住む」という文を考えます。いま、この文が「東京都」「に」「住む」というようにトークナイザーによって分割されたとします。この場合、「東京都」という語で検索を行った場合はヒットします。しかし、「京都」という語で検索を行った場合はヒットしません。そのためトークナイザーの選択は重要です。</p>
      <p>Fess の場合デフォルトでは StandardTokenizer+CJKBigramFilter が使用されていますが、schema.xml の analyzer 部分を設定することでトークナイザーを変更することができます。</p>

    <subsection name="CJKBigramFilterについて">
      <p>StandardTokenizer+CJKBigramFilter は日本語のようなマルチバイトの文字列に対しては bi-gram 、つまり二文字ずつインデックスを作成します。この場合、1文字の語を検索することはできません。</p>
    </subsection>

    <subsection name="StandardTokenizerについて">
      <p>StandardTokenizer は日本語のようなマルチバイトの文字列に対しては uni-gram 、つまり一文字ずつインデックスを作成します。そのため、検索漏れが少なくなります。また、CJKTokenizerの場合、1文字のクエリを検索することができませんが、StandardTokenizerを使用すると検索可能になります。しかし、インデックスサイズが増えるので注意してください。</p>
      <p>下記の例のように solr/core1/conf/schema.xml の analyzer 部分を変更することで、StandardTokenizer を使用できます。</p>
      <source><![CDATA[
<schema name="fess" version="1.4">
  <types>
    <fieldType name="text" class="solr.TextField" positionIncrementGap="100">
      <analyzer type="index">
        <charFilter class="solr.MappingCharFilterFactory" mapping="mapping_ja.txt"/>
        <tokenizer class="solr.StandardTokenizerFactory"/>
  :
      </analyzer>
      <analyzer type="query">
        <charFilter class="solr.MappingCharFilterFactory" mapping="mapping_ja.txt"/>
        <tokenizer class="solr.StandardTokenizerFactory"/>
  :
]]></source>
      <p>また、webapps/fess/WEB-INF/classes/app.diconでデフォルトで有効になっているuseBigramをfalseに変更します。</p>
      <source><![CDATA[
  :
    <component name="queryHelper" class="jp.sf.fess.helper.impl.QueryHelperImpl">
        <property name="useBigram">true</property>
  :
]]></source>
      <p>設定後、Fessを再起動します。</p>
    </subsection>
    </section>

  </body>
</document>
